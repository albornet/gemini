# List all models run under this configuration
models:

  # Up to 3GB
  very_small:

    # ContactDoctor.Bio-Medical-Llama-3-2-1B-CoT-012025-GGUF (finetuned from Llama-3.2-1B-Instruct)
    - {model_path: DevQuasar/ContactDoctor.Bio-Medical-Llama-3-2-1B-CoT-012025-GGUF, quant_method: gguf, quant_scheme: Q2_K}
    - {model_path: DevQuasar/ContactDoctor.Bio-Medical-Llama-3-2-1B-CoT-012025-GGUF, quant_method: gguf, quant_scheme: Q3_K_M}
    - {model_path: DevQuasar/ContactDoctor.Bio-Medical-Llama-3-2-1B-CoT-012025-GGUF, quant_method: gguf, quant_scheme: Q4_K_M}
    - {model_path: DevQuasar/ContactDoctor.Bio-Medical-Llama-3-2-1B-CoT-012025-GGUF, quant_method: gguf, quant_scheme: Q5_K_M}
    - {model_path: DevQuasar/ContactDoctor.Bio-Medical-Llama-3-2-1B-CoT-012025-GGUF, quant_method: gguf, quant_scheme: Q6_K}
    - {model_path: DevQuasar/ContactDoctor.Bio-Medical-Llama-3-2-1B-CoT-012025-GGUF, quant_method: gguf, quant_scheme: Q8_0}
    - {model_path: DevQuasar/ContactDoctor.Bio-Medical-Llama-3-2-1B-CoT-012025-GGUF, quant_method: gguf, quant_scheme: F16}

    # LLAMA3-3B-Medical-COT (/!\ -> finetuned from Llama-3.2-1B-Instruct /!\, i.e., not 3B)
    - {model_path: mradermacher/LLAMA3-3B-Medical-COT-i1-GGUF, quant_method: gguf, quant_scheme: Q2_K}
    - {model_path: mradermacher/LLAMA3-3B-Medical-COT-i1-GGUF, quant_method: gguf, quant_scheme: Q3_K_M}
    - {model_path: mradermacher/LLAMA3-3B-Medical-COT-i1-GGUF, quant_method: gguf, quant_scheme: Q4_K_M}
    - {model_path: mradermacher/LLAMA3-3B-Medical-COT-i1-GGUF, quant_method: gguf, quant_scheme: Q5_K_M}
    - {model_path: mradermacher/LLAMA3-3B-Medical-COT-i1-GGUF, quant_method: gguf, quant_scheme: Q6_K}


  # Up to 10GB
  small:

    # Llama3-Instruct-OpenBioLLM-8B-merged -> OpenBioLLM-8B merged with Llama-3-8B-Instruct chat capabilities
    - {model_path: mradermacher/Llama3-Instruct-OpenBioLLM-8B-merged-i1-GGUF, quant_method: gguf, quant_scheme: Q2_K}
    - {model_path: mradermacher/Llama3-Instruct-OpenBioLLM-8B-merged-i1-GGUF, quant_method: gguf, quant_scheme: Q3_K_M}
    - {model_path: mradermacher/Llama3-Instruct-OpenBioLLM-8B-merged-i1-GGUF, quant_method: gguf, quant_scheme: Q4_K_M}  # really good one
    - {model_path: mradermacher/Llama3-Instruct-OpenBioLLM-8B-merged-i1-GGUF, quant_method: gguf, quant_scheme: Q5_K_M}
    - {model_path: mradermacher/Llama3-Instruct-OpenBioLLM-8B-merged-i1-GGUF, quant_method: gguf, quant_scheme: Q6_K}

    # Bio-Medical-Llama (built on top of Llama-3.1-8B-Instruct)
    - {model_path: mradermacher/Bio-Medical-Llama-3.1-8B-i1-GGUF, quant_method: gguf, quant_scheme: Q2_K}
    - {model_path: mradermacher/Bio-Medical-Llama-3.1-8B-i1-GGUF, quant_method: gguf, quant_scheme: Q3_K_M}
    - {model_path: mradermacher/Bio-Medical-Llama-3.1-8B-i1-GGUF, quant_method: gguf, quant_scheme: Q4_K_M}
    - {model_path: mradermacher/Bio-Medical-Llama-3.1-8B-i1-GGUF, quant_method: gguf, quant_scheme: Q5_K_M}
    - {model_path: mradermacher/Bio-Medical-Llama-3.1-8B-i1-GGUF, quant_method: gguf, quant_scheme: Q6_K}

    # Llama-ChatDoctor (built on top of Llama-3-8B-Instruct)
    - {model_path: mradermacher/Llama-chatDoctor-i1-GGUF, quant_method: gguf, quant_scheme: Q2_K}
    - {model_path: mradermacher/Llama-chatDoctor-i1-GGUF, quant_method: gguf, quant_scheme: Q3_K_M}
    - {model_path: mradermacher/Llama-chatDoctor-i1-GGUF, quant_method: gguf, quant_scheme: Q4_K_M}
    - {model_path: mradermacher/Llama-chatDoctor-i1-GGUF, quant_method: gguf, quant_scheme: Q5_K_M}
    - {model_path: mradermacher/Llama-chatDoctor-i1-GGUF, quant_method: gguf, quant_scheme: Q6_K}


  # # More than 10GB
  # large:
    
  #   # Other potential big models we could run
  #   - {model_path: TitanML/Llama3-OpenBioLLM-70B-AWQ-4bit, quant_method: awq}
  #   - {model_path: mradermacher/DeepSeek-R1-Distill-Qwen-32B-Medical-i1-i1-GGUF, quant_method: gguf, quant_scheme: Q4_K_M}
  #   - {model_path: mradermacher/OpenBioLLM-Llama3-70B-i1-GGUF, quant_method: gguf, quant_scheme: Q4_K_M}
  #   - {model_path: mradermacher/Llama-3-70B-UltraMedical-i1-GGUF, quant_method: gguf, quant_scheme: Q4_K_M}
