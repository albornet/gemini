# Model configuration
model_path: unsloth/Qwen3-4B-GGUF
quant_scheme: Q5_K_XL
enable_thinking: true  # for, e.g., "Qwen-4B", but not "Qwen-4B-Instruct" (never thinks) or "Qwen-4B-Thinking" (always thinks)
reasoning_parser: qwen3  # deepseek_r1, qwen3, ... /!\ use deepseek_r1 for explicitely thinking models, qwen3 otherwise
logit_processors:  # null for not using any logit processor
  - src.models.logit_processors:ThinkingBudgetProcessor
  - src.models.logit_processors:JSONParsingProcessor
max_thinking_tokens: 768  # maximum number of tokens in between <think> and </think>
output_schema_name: PatientInfoSchema  # null for not using

# Backend configuration
inference_backend: vllm-serve-async
max_concurrent_requests: 64  # client-side ("semaphore", only used for vllm-serve or vllm-serve-async)
max_concurrent_infs: 64

# Inference configuration
n_inference_repeats: 10  # number of times each inference is repeated for benchmark
use_output_guide: true
max_context_length: 10000  # note: maximum number of tokens in our documents is approximately 8200, and prompt size is about 1000 tokens
max_new_tokens: 1024  # 1024 - note: this corresponds to the maximum generated tokens
top_p: 0.90
temperature: 0.60
use_flash_attention: true  # only used for llama-cpp backend

# Run parameters
num_gpus_to_use: null  # if null, will take all available gpus by default (see run_benchmark.sh)
gpu_memory_utilization: 0.80
skip_to_next_model_if_error: true
delete_model_cache_after_run: true
