# List all models run under this configuration
models:

  # Up to 3GB
  very_small:

    # Qwen2.5-0.5B-Instruct
    - {model_path: bartowski/Qwen2.5-0.5B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q2_K}
    - {model_path: bartowski/Qwen2.5-0.5B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q3_K_M}
    - {model_path: bartowski/Qwen2.5-0.5B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q4_K_M}
    - {model_path: bartowski/Qwen2.5-0.5B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q5_K_M}
    - {model_path: bartowski/Qwen2.5-0.5B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q6_K}
    - {model_path: bartowski/Qwen2.5-0.5B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q8_0}
    - {model_path: bartowski/Qwen2.5-0.5B-Instruct-GGUF, quant_method: gguf, quant_scheme: F16}

    # Qwen2.5-1.5B-Instruct
    - {model_path: bartowski/Qwen2.5-1.5B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q2_K}
    - {model_path: bartowski/Qwen2.5-1.5B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q3_K_M}
    - {model_path: bartowski/Qwen2.5-1.5B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q4_K_M}
    - {model_path: bartowski/Qwen2.5-1.5B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q5_K_M}
    - {model_path: bartowski/Qwen2.5-1.5B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q6_K}
    - {model_path: bartowski/Qwen2.5-1.5B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q8_0}
    - {model_path: bartowski/Qwen2.5-1.5B-Instruct-GGUF, quant_method: gguf, quant_scheme: F16}

    # Qwen2.5-3B-Instruct
    - {model_path: bartowski/Qwen2.5-3B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q2_K}
    - {model_path: bartowski/Qwen2.5-3B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q3_K_M}
    - {model_path: bartowski/Qwen2.5-3B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q4_K_M}
    - {model_path: bartowski/Qwen2.5-3B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q5_K_M}
    - {model_path: bartowski/Qwen2.5-3B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q6_K}
    - {model_path: bartowski/Qwen2.5-3B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q8_0}
    - {model_path: bartowski/Qwen2.5-3B-Instruct-GGUF, quant_method: gguf, quant_scheme: F16}


  # Up to 10GB
  small:

    # Qwen2.5-7B-Instruct
    - {model_path: bartowski/Qwen2.5-7B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q2_K}
    - {model_path: bartowski/Qwen2.5-7B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q3_K_M}
    - {model_path: bartowski/Qwen2.5-7B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q4_K_M}
    - {model_path: bartowski/Qwen2.5-7B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q5_K_M}
    - {model_path: bartowski/Qwen2.5-7B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q6_K}
    - {model_path: bartowski/Qwen2.5-7B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q8_0}

    # Qwen2.5-14B-Instruct (quantizations I can run on my 12G GPU)
    - {model_path: bartowski/Qwen2.5-14B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q2_K}
    - {model_path: bartowski/Qwen2.5-14B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q3_K_M}


  # More than 10B
  large:

    # Qwen2.5-14B-Instruct
    - {"model_path": "bartowski/Qwen2.5-14B-Instruct-GGUF", "quant": "gguf", "quant_scheme": "Q2_K"}
    - {"model_path": "bartowski/Qwen2.5-14B-Instruct-GGUF", "quant": "gguf", "quant_scheme": "Q3_K_M"}
    - {"model_path": "bartowski/Qwen2.5-14B-Instruct-GGUF", "quant": "gguf", "quant_scheme": "Q4_K_M"}
    - {"model_path": "bartowski/Qwen2.5-14B-Instruct-GGUF", "quant": "gguf", "quant_scheme": "Q5_K_M"}
    - {"model_path": "bartowski/Qwen2.5-14B-Instruct-GGUF", "quant": "gguf", "quant_scheme": "Q6_K"}
    - {"model_path": "bartowski/Qwen2.5-14B-Instruct-GGUF", "quant": "gguf", "quant_scheme": "Q8_0"}
    - {"model_path": "bartowski/Qwen2.5-14B-Instruct-GGUF", "quant": "gguf", "quant_scheme": "F16"}

    # Qwen2.5-32B-Instruct
    - {"model_path": "bartowski/Qwen2.5-32B-Instruct-GGUF", "quant": "gguf", "quant_scheme": "Q2_K"}
    - {"model_path": "bartowski/Qwen2.5-32B-Instruct-GGUF", "quant": "gguf", "quant_scheme": "Q3_K_M"}
    - {"model_path": "bartowski/Qwen2.5-32B-Instruct-GGUF", "quant": "gguf", "quant_scheme": "Q4_K_M"}
    - {"model_path": "bartowski/Qwen2.5-32B-Instruct-GGUF", "quant": "gguf", "quant_scheme": "Q5_K_M"}
    - {"model_path": "bartowski/Qwen2.5-32B-Instruct-GGUF", "quant": "gguf", "quant_scheme": "Q6_K"}
    - {"model_path": "bartowski/Qwen2.5-32B-Instruct-GGUF", "quant": "gguf", "quant_scheme": "Q8_0"}
    - {"model_path": "bartowski/Qwen2.5-32B-Instruct-GGUF", "quant": "gguf", "quant_scheme": "F16"}

    # Qwen2.5-72B-Instruct
    - {"model_path": "bartowski/Qwen2.5-72B-Instruct-GGUF", "quant": "gguf", "quant_scheme": "Q2_K"}
    - {"model_path": "bartowski/Qwen2.5-72B-Instruct-GGUF", "quant": "gguf", "quant_scheme": "Q3_K_M"}
    - {"model_path": "bartowski/Qwen2.5-72B-Instruct-GGUF", "quant": "gguf", "quant_scheme": "Q4_K_M"}
    - {"model_path": "bartowski/Qwen2.5-72B-Instruct-GGUF", "quant": "gguf", "quant_scheme": "Q5_K_M"}
    - {"model_path": "bartowski/Qwen2.5-72B-Instruct-GGUF", "quant": "gguf", "quant_scheme": "Q6_K"}
    - {"model_path": "bartowski/Qwen2.5-72B-Instruct-GGUF", "quant": "gguf", "quant_scheme": "Q8_0"}
    - {"model_path": "bartowski/Qwen2.5-72B-Instruct-GGUF", "quant": "gguf", "quant_scheme": "F16"}
