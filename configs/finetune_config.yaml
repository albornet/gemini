base_model_path: "meta-llama/Llama-3-8B-Instruct"
new_model_path: "./models/llama3-8b-mrs-finetuned"

lora_config:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"

training_args:
  num_train_epochs: 3
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-4
  optim: "paged_adamw_8bit"
  logging_steps: 25
  save_steps: 100
  fp16: false
  bf16: true
  max_grad_norm: 0.3
  warmup_ratio: 0.03
  lr_scheduler_type: "constant"