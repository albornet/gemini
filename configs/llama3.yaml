# List all models run under this configuration
models:

  # Up to 3GB
  very_small:

    # Llama-3.2-1B-Instruct (actually quantized with i1)
    # - {model_path: MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q2_K}
    - {model_path: MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q3_K_M}
    - {model_path: MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q4_K_M}
    - {model_path: MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q5_K_M}
    - {model_path: MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q6_K}
    - {model_path: MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q8_0}
    - {model_path: MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF, quant_method: gguf, quant_scheme: FP16}

    # Llama-3.2-3B-Instruct (actually quantized with i1)
    - {model_path: MaziyarPanahi/Llama-3.2-3B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q2_K}
    - {model_path: MaziyarPanahi/Llama-3.2-3B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q3_K_M}
    - {model_path: MaziyarPanahi/Llama-3.2-3B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q4_K_M}
    - {model_path: MaziyarPanahi/Llama-3.2-3B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q5_K_M}
    - {model_path: MaziyarPanahi/Llama-3.2-3B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q6_K}
    - {model_path: MaziyarPanahi/Llama-3.2-3B-Instruct-GGUF, quant_method: gguf, quant_scheme: Q8_0}
    - {model_path: MaziyarPanahi/Llama-3.2-3B-Instruct-GGUF, quant_method: gguf, quant_scheme: FP16}


  # Up to 10GB
  small:

    # Llama-3.1-8B-Instruct
    - {model_path: mradermacher/Meta-Llama-3.1-8B-Instruct-i1-GGUF, quant_method: gguf, quant_scheme: Q2_K}
    - {model_path: mradermacher/Meta-Llama-3.1-8B-Instruct-i1-GGUF, quant_method: gguf, quant_scheme: Q3_K_M}
    - {model_path: mradermacher/Meta-Llama-3.1-8B-Instruct-i1-GGUF, quant_method: gguf, quant_scheme: Q4_K_M}
    - {model_path: mradermacher/Meta-Llama-3.1-8B-Instruct-i1-GGUF, quant_method: gguf, quant_scheme: Q5_K_M}
    - {model_path: mradermacher/Meta-Llama-3.1-8B-Instruct-i1-GGUF, quant_method: gguf, quant_scheme: Q6_K}
