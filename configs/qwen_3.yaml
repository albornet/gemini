# List all models run under this configuration
models:

  ###################################################################
  # Note: Qwen3 models are the evolution of Qwen2.5-Instruct models #
  ###################################################################

  # Up to 3GB
  very_small:

    # Qwen3-0.6B
    - {model_path: unsloth/Qwen3-0.6B-GGUF, quant_method: gguf, quant_scheme: Q1_M}
    - {model_path: unsloth/Qwen3-0.6B-GGUF, quant_method: gguf, quant_scheme: Q2_K}
    - {model_path: unsloth/Qwen3-0.6B-GGUF, quant_method: gguf, quant_scheme: Q3_K_M}
    - {model_path: unsloth/Qwen3-0.6B-GGUF, quant_method: gguf, quant_scheme: Q4_K_M}
    - {model_path: unsloth/Qwen3-0.6B-GGUF, quant_method: gguf, quant_scheme: Q5_K_M}
    - {model_path: unsloth/Qwen3-0.6B-GGUF, quant_method: gguf, quant_scheme: Q6_K}
    - {model_path: unsloth/Qwen3-0.6B-GGUF, quant_method: gguf, quant_scheme: Q8_0}
    - {model_path: unsloth/Qwen3-0.6B-GGUF, quant_method: gguf, quant_scheme: BF16}

    # Qwen3-1.7B
    - {model_path: unsloth/Qwen3-1.7B-GGUF, quant_method: gguf, quant_scheme: Q1_M}
    - {model_path: unsloth/Qwen3-1.7B-GGUF, quant_method: gguf, quant_scheme: Q2_K}
    - {model_path: unsloth/Qwen3-1.7B-GGUF, quant_method: gguf, quant_scheme: Q3_K_M}
    - {model_path: unsloth/Qwen3-1.7B-GGUF, quant_method: gguf, quant_scheme: Q4_K_M}
    - {model_path: unsloth/Qwen3-1.7B-GGUF, quant_method: gguf, quant_scheme: Q5_K_M}
    - {model_path: unsloth/Qwen3-1.7B-GGUF, quant_method: gguf, quant_scheme: Q6_K}
    - {model_path: unsloth/Qwen3-1.7B-GGUF, quant_method: gguf, quant_scheme: Q8_0}
    - {model_path: unsloth/Qwen3-1.7B-GGUF, quant_method: gguf, quant_scheme: BF16}

    # Qwen3-4B (intense quantizations)
    - {model_path: unsloth/Qwen3-4B-GGUF, quant_method: gguf, quant_scheme: Q1_M}  # 1GB
    - {model_path: unsloth/Qwen3-4B-GGUF, quant_method: gguf, quant_scheme: Q2_K}  # 1.5GB
    - {model_path: unsloth/Qwen3-4B-GGUF, quant_method: gguf, quant_scheme: Q3_K_M}  # 2GB
    - {model_path: unsloth/Qwen3-4B-GGUF, quant_method: gguf, quant_scheme: Q4_K_M}  # 2.5GB
    - {model_path: unsloth/Qwen3-4B-GGUF, quant_method: gguf, quant_scheme: Q5_K_M}  # 3GB


  # Up to 10GB
  small:

    # Qwen3-4B (less intense quantizations)
    - {model_path: unsloth/Qwen3-4B-GGUF, quant_method: gguf, quant_scheme: Q6_K}  # 3.5GB
    - {model_path: unsloth/Qwen3-4B-GGUF, quant_method: gguf, quant_scheme: Q8_0}  # 5GB
    - {model_path: unsloth/Qwen3-4B-GGUF, quant_method: gguf, quant_scheme: BF16}  # 8GB

    # Qwen3-8B (intense quantizations)
    - {model_path: unsloth/Qwen3-8B-GGUF, quant_method: gguf, quant_scheme: Q1_M}  # 2.5GB
    - {model_path: unsloth/Qwen3-8B-GGUF, quant_method: gguf, quant_scheme: Q2_K}  # 3.5GB
    - {model_path: unsloth/Qwen3-8B-GGUF, quant_method: gguf, quant_scheme: Q3_K_M}  # 4GB
    - {model_path: unsloth/Qwen3-8B-GGUF, quant_method: gguf, quant_scheme: Q4_K_M}  # 5GB
    - {model_path: unsloth/Qwen3-8B-GGUF, quant_method: gguf, quant_scheme: Q5_K_M}  # 6GB
    - {model_path: unsloth/Qwen3-8B-GGUF, quant_method: gguf, quant_scheme: Q6_K}  # 8GB

    # Qwen3-14B (intense quantizations)
    - {model_path: unsloth/Qwen3-14B-GGUF, quant_method: gguf, quant_scheme: Q1_M}  # 4GB
    - {model_path: unsloth/Qwen3-14B-GGUF, quant_method: gguf, quant_scheme: Q2_K}  # 6GB
    - {model_path: unsloth/Qwen3-14B-GGUF, quant_method: gguf, quant_scheme: Q3_K_M}  # 7GB
    - {model_path: unsloth/Qwen3-14B-GGUF, quant_method: gguf, quant_scheme: Q4_K_M}  # 9GB


  # More than 10GB
  large:

    # Qwen3-14B (less intense quantizations)
    - {model_path: unsloth/Qwen3-8B-GGUF, quant_method: gguf, quant_scheme: Q8_0}  # 11GB
    - {model_path: unsloth/Qwen3-8B-GGUF, quant_method: gguf, quant_scheme: BF16}  # 17GB

    # Qwen3-14B (less intense quantizations)
    - {model_path: unsloth/Qwen3-14B-GGUF, quant_method: gguf, quant_scheme: Q5_K_M}  # 11GB
    - {model_path: unsloth/Qwen3-14B-GGUF, quant_method: gguf, quant_scheme: Q6_K}  # 12GB
    - {model_path: unsloth/Qwen3-14B-GGUF, quant_method: gguf, quant_scheme: Q8_0}  # 20GB
    - {model_path: unsloth/Qwen3-14B-GGUF, quant_method: gguf, quant_scheme: BF16}  # 30GB

    # Qwen3-32B
    - {model_path: unsloth/Qwen3-32B-GGUF, quant_method: gguf, quant_scheme: Q1_M}  # 8GB
    - {model_path: unsloth/Qwen3-32B-GGUF, quant_method: gguf, quant_scheme: Q2_M}  # 12GB
    - {model_path: unsloth/Qwen3-32B-GGUF, quant_method: gguf, quant_scheme: Q3_K_M}  # 16GB
    - {model_path: unsloth/Qwen3-32B-GGUF, quant_method: gguf, quant_scheme: Q4_K_M}  # 20GB
    - {model_path: unsloth/Qwen3-32B-GGUF, quant_method: gguf, quant_scheme: Q5_K_M}  # 23GB
    - {model_path: unsloth/Qwen3-32B-GGUF, quant_method: gguf, quant_scheme: Q6_K}  # 30GB
    - {model_path: unsloth/Qwen3-32B-GGUF, quant_method: gguf, quant_scheme: Q8_0}  # 40GB
    - {model_path: unsloth/Qwen3-32B-GGUF, quant_method: gguf, quant_scheme: BF16}  # 66GB
